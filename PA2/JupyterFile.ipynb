{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python393jvsc74a57bd0d480ba82f8eae1be4696d10170c3a9141a57978f2c0cd764de7397b5a84b808e",
   "display_name": "Python 3.9.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('data/mushroom.train', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "xtrn = data[:, 1:]\n",
    "ytrn = data[: , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lena = len(xtrn)\n",
    "w = np.ones((lena, 1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: (1, {(1, 4, False): {(2, 1, False): 0, (2, 1, True): 1}, (1, 4, True): 0}), 1: (1, {(1, 1, False): {(1, 4, False): 0, (1, 4, True): 0}, (1, 1, True): 0}), 2: (1, {(1, 1, False): {(1, 4, False): 0, (1, 4, True): 0}, (1, 1, True): 0}), 3: (1, {(1, 1, False): {(1, 4, False): 0, (1, 4, True): 0}, (1, 1, True): 0}), 4: (1, {(1, 4, False): {(2, 1, False): 0, (2, 1, True): 1}, (1, 4, True): 0}), 5: (1, {(1, 1, False): {(1, 4, False): 0, (1, 4, True): 0}, (1, 1, True): 0}), 6: (1, {(1, 1, False): {(1, 4, False): 0, (1, 4, True): 0}, (1, 1, True): 0}), 7: (1, {(1, 1, False): {(1, 4, False): 0, (1, 4, True): 0}, (1, 1, True): 0}), 8: (1, {(1, 4, False): {(2, 1, False): 0, (2, 1, True): 1}, (1, 4, True): 0}), 9: (1, {(1, 1, False): {(1, 4, False): 0, (1, 4, True): 0}, (1, 1, True): 0})}\n"
     ]
    }
   ],
   "source": [
    "# decision_tree.py\n",
    "# ---------\n",
    "# Licensing Information:  You are free to use or extend these projects for\n",
    "# personal and educational purposes provided that (1) you do not distribute\n",
    "# or publish solutions, (2) you retain this notice, and (3) you provide clear\n",
    "# attribution to UT Dallas, including a link to http://cs.utdallas.edu.\n",
    "#\n",
    "# This file is part of Programming Assignment 1 for CS6375: Machine Learning.\n",
    "# Gautam Kunapuli (gautam.kunapuli@utdallas.edu)\n",
    "# Sriraam Natarajan (sriraam.natarajan@utdallas.edu),\n",
    "#\n",
    "#\n",
    "# INSTRUCTIONS:\n",
    "# ------------\n",
    "# 1. This file contains a skeleton for implementing the ID3 algorithm for\n",
    "# Decision Trees. Insert your code into the various functions that have the\n",
    "# comment \"INSERT YOUR CODE HERE\".\n",
    "#\n",
    "# 2. Do NOT modify the classes or functions that have the comment \"DO NOT\n",
    "# MODIFY THIS FUNCTION\".\n",
    "#\n",
    "# 3. Do not modify the function headers for ANY of the functions.\n",
    "#\n",
    "# 4. You may add any other helper functions you feel you may need to print,\n",
    "# visualize, test, or save the data and results. However, you MAY NOT utilize\n",
    "# the package scikit-learn OR ANY OTHER machine learning package in THIS file.\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "# Scikit learn is used only for drawing confusion matrix.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Matplotlib is used only for graph plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def partition(x):\n",
    "    \"\"\"\n",
    "    Partition the column vector x into subsets indexed by its unique values (v1, ... vk)\n",
    "\n",
    "    Returns a dictionary of the form\n",
    "    { v1: indices of x == v1,\n",
    "      v2: indices of x == v2,\n",
    "      ...\n",
    "      vk: indices of x == vk }, where [v1, ... vk] are all the unique values in the vector z.\n",
    "    \"\"\"\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # raise Exception('Function not yet implemented!')\n",
    "    unique_values = np.unique(x)\n",
    "    my_dict = {}\n",
    "    for i in unique_values:\n",
    "        my_dict[i] = np.where(x==i)\n",
    "           \n",
    "    return my_dict\n",
    "    \n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Compute the entropy of a vector y by considering the counts of the unique values (v1, ... vk), in z\n",
    "\n",
    "    Returns the entropy of z: H(z) = p(z=v1) log2(p(z=v1)) + ... + p(z=vk) log2(p(z=vk))\n",
    "    \"\"\"\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # raise Exception('Function not yet implemented!')\n",
    "    (unique_vals, counts) = np.unique(y, return_counts=True)\n",
    "    total_vals = sum(counts)\n",
    "    probs = counts/total_vals\n",
    "    logs = -np.log2(probs)\n",
    "    ents = probs*logs\n",
    "    return sum(ents)\n",
    "\n",
    "\n",
    "\n",
    "def mutual_information(x, y):\n",
    "    \"\"\"\n",
    "    Compute the mutual information between a data column (x) and the labels (y). The data column is a single attribute\n",
    "    over all the examples (n x 1). Mutual information is the difference between the entropy BEFORE the split set, and\n",
    "    the weighted-average entropy of EACH possible split.\n",
    "\n",
    "    Returns the mutual information: I(x, y) = H(y) - H(y | x)\n",
    "    \"\"\"\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # raise Exception('Function not yet implemented!')\n",
    "    entY = entropy(y)\n",
    "    hashmap = {}\n",
    "    for i in np.unique(x):\n",
    "        hashmap[i] = dict(Counter(np.array(y[np.where(x==i)]).flatten()))\n",
    "\n",
    "    total_ent = 0   \n",
    "    for j in hashmap:\n",
    "        total_ent += entropy_calculation_from_dictionary(hashmap[j], len(y))\n",
    "\n",
    "    return entY - total_ent\n",
    "\n",
    "\n",
    "def entropy_calculation_from_dictionary(my_dict, total_labels):\n",
    "    total_sum = sum(my_dict.values())\n",
    "    res = 0\n",
    "    for i in my_dict:\n",
    "        frac = (my_dict[i]/total_sum)\n",
    "        res -= frac * np.log2(frac)\n",
    "\n",
    "    res = (total_sum/total_labels)*res\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def id3(x, y, attribute_value_pairs=None, depth=0, max_depth=5):\n",
    "    \"\"\"\n",
    "    Implements the classical ID3 algorithm given training data (x), training labels (y) and an array of\n",
    "    attribute-value pairs to consider. This is a recursive algorithm that depends on three termination conditions\n",
    "        1. If the entire set of labels (y) is pure (all y = only 0 or only 1), then return that label\n",
    "        2. If the set of attribute-value pairs is empty (there is nothing to split on), then return the most common\n",
    "           value of y (majority label)\n",
    "        3. If the max_depth is reached (pre-pruning bias), then return the most common value of y (majority label)\n",
    "    Otherwise the algorithm selects the next best attribute-value pair using INFORMATION GAIN as the splitting criterion\n",
    "    and partitions the data set based on the values of that attribute before the next recursive call to ID3.\n",
    "\n",
    "    The tree we learn is a BINARY tree, which means that every node has only two branches. The splitting criterion has\n",
    "    to be chosen from among all possible attribute-value pairs. That is, for a problem with two features/attributes x1\n",
    "    (taking values a, b, c) and x2 (taking values d, e), the initial attribute value pair list is a list of all pairs of\n",
    "    attributes with their corresponding values:\n",
    "    [(x1, a),\n",
    "     (x1, b),\n",
    "     (x1, c),\n",
    "     (x2, d),\n",
    "     (x2, e)]\n",
    "     If we select (x2, d) as the best attribute-value pair, then the new decision node becomes: [ (x2 == d)? ] and\n",
    "     the attribute-value pair (x2, d) is removed from the list of attribute_value_pairs.\n",
    "\n",
    "    The tree is stored as a nested dictionary, where each entry is of the form\n",
    "                    (attribute_index, attribute_value, True/False): subtree\n",
    "    * The (attribute_index, attribute_value) determines the splitting criterion of the current node. For example, (4, 2)\n",
    "    indicates that we test if (x4 == 2) at the current node.\n",
    "    * The subtree itself can be nested dictionary, or a single label (leaf node).\n",
    "    * Leaf nodes are (majority) class labels\n",
    "\n",
    "    Returns a decision tree represented as a nested dictionary, for example\n",
    "    {(4, 1, False):\n",
    "        {(0, 1, False):\n",
    "            {(1, 1, False): 1,\n",
    "             (1, 1, True): 0},\n",
    "         (0, 1, True):\n",
    "            {(1, 1, False): 0,\n",
    "             (1, 1, True): 1}},\n",
    "     (4, 1, True): 1}\n",
    "    \"\"\"\n",
    "    # INSERT YOUR CODE HERE. NOTE: THIS IS A RECURSIVE FUNCTION.\n",
    "    # raise Exception('Function not yet implemented!')\n",
    "    unique_labels, count_unique_labels = np.unique(y, return_counts = True)\n",
    "    \n",
    "    if attribute_value_pairs is None:\n",
    "        attribute_value_pairs = []\n",
    "        for i in range(x.shape[1]):\n",
    "            unique_attributes = partition(x[:, i])\n",
    "            for each_attribute_from_set in unique_attributes.keys():\n",
    "                attribute_value_pairs.append((i, each_attribute_from_set))\n",
    "    attribute_value_pairs = np.array(attribute_value_pairs).astype(int)\n",
    "    \n",
    "    if len(unique_labels)==1:\n",
    "        return unique_labels[0]\n",
    "    \n",
    "    if len(attribute_value_pairs)==0 or depth == max_depth:\n",
    "        return unique_labels[np.argmax(count_unique_labels)]\n",
    "\n",
    "    entropy_info = []\n",
    "\n",
    "    for feature_column, value in attribute_value_pairs:\n",
    "        indices = np.where(x[:, feature_column] == value)[0] \n",
    "        y_for_feature_single_attribute = y[indices] \n",
    "        entropy_info_for_feature_single_attribute = entropy(y_for_feature_single_attribute)\n",
    "        entropy_info.append(entropy_info_for_feature_single_attribute)\n",
    "\n",
    "    entropy_info_array = np.array(entropy_info)\n",
    "    (max_attribute, max_value) = attribute_value_pairs[np.argmin(entropy_info_array)]\n",
    "    max_attribute_partition = partition(np.array(x[:, max_attribute] == max_value).astype(int))\n",
    "    attribute_value_pairs = np.delete(attribute_value_pairs, np.argwhere(np.all(attribute_value_pairs == (max_attribute, max_value), axis=1)),0)\n",
    "\n",
    "    decision_tree = {}\n",
    "\n",
    "    for decision_value, indices in max_attribute_partition.items():\n",
    "        x_new = x[indices]\n",
    "        y_new = y[indices]\n",
    "        attribute_decision = bool(decision_value)\n",
    "\n",
    "        decision_tree[(max_attribute, max_value, attribute_decision)] = id3(x_new, y_new, attribute_value_pairs=attribute_value_pairs, max_depth=max_depth, depth=depth+1)\n",
    "\n",
    "    return decision_tree\n",
    "    \n",
    "\n",
    "\n",
    "def predict_example(x, tree):\n",
    "    \"\"\"\n",
    "    Predicts the classification label for a single example x using tree by recursively descending the tree until\n",
    "    a label/leaf node is reached.\n",
    "\n",
    "    Returns the predicted label of x according to tree\n",
    "    \"\"\"\n",
    "    # INSERT YOUR CODE HERE. NOTE: THIS IS A RECURSIVE FUNCTION.\n",
    "    # raise Exception('Function not yet implemented!')\n",
    "    for attribute_keys, sub_tree in tree.items():\n",
    "        attribute = attribute_keys[0]\n",
    "        value = attribute_keys[1]\n",
    "        decision = attribute_keys[2]\n",
    "\n",
    "        if decision == (x[attribute] == value):\n",
    "            if type(sub_tree) is dict:\n",
    "                label = predict_example(x, sub_tree)\n",
    "            else:\n",
    "                label = sub_tree\n",
    "\n",
    "            return label\n",
    "    \n",
    "\n",
    "\n",
    "def compute_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the average error between the true labels (y_true) and the predicted labels (y_pred)\n",
    "\n",
    "    Returns the error = (1/n) * sum(y_true != y_pred)\n",
    "    \"\"\"\n",
    "    # INSERT YOUR CODE HERE\n",
    "    sum=0\n",
    "    n=len(y_true)\n",
    "    for x in range(n):\n",
    "        if(y_true[x]!=y_pred[x]):\n",
    "            sum= sum+1\n",
    "    err=sum/n\n",
    "    return err\n",
    "\n",
    "\n",
    "def visualize(tree, depth=0):\n",
    "    \"\"\"\n",
    "    Pretty prints (kinda ugly, but hey, it's better than nothing) the decision tree to the console. Use print(tree) to\n",
    "    print the raw nested dictionary representation.\n",
    "    DO NOT MODIFY THIS FUNCTION!\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        print('TREE')\n",
    "\n",
    "    for index, split_criterion in enumerate(tree):\n",
    "        sub_trees = tree[split_criterion]\n",
    "\n",
    "        # Print the current node: split criterion\n",
    "        print('|\\t' * depth, end='')\n",
    "        print('+-- [SPLIT: x{0} = {1}]'.format(split_criterion[0], split_criterion[1]))\n",
    "\n",
    "        # Print the children\n",
    "        if type(sub_trees) is dict:\n",
    "            visualize(sub_trees, depth + 1)\n",
    "        else:\n",
    "            print('|\\t' * (depth + 1), end='')\n",
    "            print('+-- [LABEL = {0}]'.format(sub_trees))\n",
    "\n",
    "def bagging(x, y, max_depth, num_trees):\n",
    "    \"\"\" Bagging function taking in x, y, max_dept \n",
    "    is sent to id3 and looped over num_trees (bag size) \n",
    "    returns weighted pair of hypotheses\"\"\"\n",
    "\n",
    "    hypotheses = {}\n",
    "    # attribute_idx = np.array(range(data.dim))\n",
    "    # generating attributes\n",
    "    attributes = []\n",
    "    cols  = x.shape[1]\n",
    "    for i in range(cols):\n",
    "        arr = np.unique(x[:, i])\n",
    "        for value in arr:\n",
    "            attributes.append((i, value))\n",
    "    lena = len(x)\n",
    "    # initializing weights to 1 for boosting\n",
    "    alpha = 1\n",
    "    w = np.ones((lena, 1), dtype=int)    \n",
    "    # iterating over j number of trees\n",
    "    for j in range(num_trees):\n",
    "        # generating a random array of indicies with replacement over the length of x\n",
    "        new_array = np.random.choice(lena,size =lena,replace=True)\n",
    "        #calling id3 over the indices of the new array\n",
    "        tree = id3(x[new_array],y[new_array],attributes, max_depth)\n",
    "        # appending to a global tree as a weighted pair\n",
    "        hypotheses[j] = (alpha, tree)\n",
    "    print(hypotheses)\n",
    "    return hypotheses\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the training data\n",
    "    M = np.genfromtxt('data/mushroom.train', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "    ytrn = M[:, 0]\n",
    "    Xtrn = M[:, 1:]\n",
    "\n",
    "    # Load the test data\n",
    "    M = np.genfromtxt('data/mushroom.test', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "    ytst = M[:, 0]\n",
    "    Xtst = M[:, 1:]\n",
    "\n",
    "\n",
    "    hypotheses = bagging(Xtrn, ytrn, 3, 10)\n",
    "    # Learn a decision tree of depth 3\n",
    "    # decision_tree = id3(Xtrn, ytrn, max_depth=2)\n",
    "    # visualize(decision_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "seed = 8\n",
    "kfold = model_selection.KFold(n_splits=3, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_class = DecisionTreeClassifier()\n",
    "model = BaggingClassifier(base_estimator=base_class, n_estimators=10, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, xtrn, ytrn, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.999507631708518"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}